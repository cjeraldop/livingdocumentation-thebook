# Information Consolidation

Sometime the knowledge is spread over many place: a type hierarchy with an interface and 5 implementing classes is actually declared in 6 different files. The content of a package or module is actually stored in many files. The full list of dependencies of the project is actually defined partially in its Maven manifest (POM), and also in its parent manifest.

This means that there's a need to collect and aggregate many little bits of knowledge in order to get a full picture. For example, the total number of public methods of a module is the sum of the number of public methods of each of its classes, and the big picture of a system is basically the union of the black-box view of each of its part. We say that the overall knowledge is derived by a consolidation mechanism.

Even if the knowledge is split in many little parts, it's still desirable to consider all these little bits as the authoritative single sources of truth. The derived consolidated knowledge is therefore a special case of published document extracted from many places.

![From fragmented authoritative knowledge to unified knowledge](images/consolidation_mechanism.jpg)

**Therefore: Design a simple mechanism to automatically consolidate all the disparate information from many places. This mechanism must be ran as often as necessary to ensure that the information about the whole are up-to-ate with respect to the parts. Avoid any storage of the consolidated information, except for technical concerns like caching.**

## How it works

Basically a consolidation is like a SQL Group By. You take many things with some properties in common, and you find a way to turn this plural into an equivalent singular. In practice it's done by scanning every element within a given perimeter, while growing the result.

For example, to reconstitute the full picture of a class hierarchy within the limits of one project from its individual elements, it's necessary to scan just every class and every interface of the project. The scanning process keeps a growing dictionary of every hierarchy under construction so far in the process, for example with a mapping *top of hierarchy -> list of subclasses*. Every time it encounters a class that extends another or that extends an interface, it adds it to the dictionary.

When the scan is done, the dictionary contains a list of all type hierarchies of the project. Of course it's possible to reduce the process only to a subset of these hierarchies of interest for a particular documentation need, like restrict the scan only to classes and interfaces that belong to a published API.

As another example, if we want to create a blackbox living diagram of a system made of smaller components that each have their own set of inputs and outputs, we want to do the following:

![The blackbox view of the whole system can be derived by a consolidation of the blackbox view of its components](images/ConsolidatedView.jpg)

In its simplest form, the consolidation can just collect the union of every input and output from each component. In a more sophisticated approach, it can try to remove every input and output that match to each other internally. It's up to you to decide how you want it to happen for a particular need.

## Implementation remarks

As usual, the first idea must be to reuse a tool that already can do the desired consolidation. Some parsers for Java code can provide type hierarchies for example. If what you want is not there, you can add it, for example by writing another visitor on the programming language AST. Some more powerful tools even provide a their own language to query the code base very efficiently. In this idea, you may want to load the AST into a graph database if you have to do very complex queries. But if you begin to do that, I'm afraid you're becoming a software vendor of documentation tools.

If the derived knowledge is kept in cache for performance issues, make sure it does not become a source of truth, and that it can always be properly dropped then rebuilt from scratch from all the sources of truth.

For most systems it is possible to scan all parts in sequence in a batch processing fashion. This is typically done during a build, and produces the consolidation ready for publication on the project website or as a report.

For large systems like an information system, it is not practical to run calculations scanning all parts in sequence. In this case the consolidation process may be done incrementally. For example the build of each part can contribute a partial update by pushing data to an overall consolidation state somewhere in a shared place like a shared database. This consolidation state is derived information, it is less trusted than the information from each build. If anything goes wrong, drop it and let it grow again from the contributions of each build.
