# Introducing Living Documentation by example


This real-world example is about batches to export credit authorizations from one application to external systems. Members of the team stay less than 3 years in average, therefore the need for some documentation is not controversial here. The team and the managers heard about Living Documentation, they're interested so we eventually spend one hour discussing what could be done.

When considering what to do, we try to focus on everything that should be documented in order to improve the life of the development team. Then by looking at the current state of the available documentation, we can then propose actions to better manage the knowledge.

Currently, there are some documents but they are out of date and not reliable. We usually have to ask the most knowledgeable team member all the time to get the knowledge needed to perform any task.

There's a lot of potential for improvement, including some quick wins. We could introduce all the items below to start a Living Documentation journey.

## README and Ready-Made Documentation

The source code repository does not have a README file at its root. Therefore, first add a `README` file at the root of the module.

In this README file, mention clearly that this module follows the **Data Pump pattern** with a brief explanation and a link to a reference on the web. From a Living Documentation perspective, we are referencing a READY-MADE documentation.

To be more useful, we can elaborate a little bit on the data pump with a description of its main 'parameters' in the `README` file:

- **Target system & format**: Company standard XML dialect
- **Governance**: this data pump belongs to the *Spartacus Credit Approval* component and is managed as part of it
- **Rationale**: The Data Pump pattern is chosen over more standard integration through services endpoints because the target system imposes a **bulk** integration style, with lots of data to transfer between the two systems daily.

All this remains a bit abstract, so it's desirable to include in the `README` file a link to a folder containing some sample files describing the inputs and outputs of the component:

     Sample input and output files can be found in '/samples/' (with a link to 'target/doc/samples')

## Business Behavior

The core complexity of the module is the determination of eligibility. It is best described by business scenarios, already partially automated in *Cucumber JVM*.

We can reuse some of these scenarios to generate the sample files mentioned before. This way the sample files will remain up-to-date.

Having business-readable scenarios is nice, but here we need to make these scenarios accessible to non-developers. The basic *Cucumber Report* can show the scenarios as a web page online. You may consider the alternative tool *Pickles* for the living documentation to be available online to anyone in a better form and with a search engine.

## Visible Workings and Single Source of Truth

The transcoding used to generate the XML report is defined in code, and in an Excel file as well:

     | input field name | output field name | formatter           |
     | trade date       | TrdDate           | ukToUsDateFormatter |

We realize there is duplication of knowledge for no particular benefit here. Who's the authority in case of disagreement? Usually it should be the spreadsheet file, but after a while it will be the code.

We could improve that situation by deciding that the spreadsheet file is the single source of truth (aka the Golden Source). The code then parses this file and interprets it to drive its behavior. In this approach, the file is directly its own documentation.

For example in pseudo-code:

       For each input field declared in a generic data dictionary
         Fetch the value from the input field
         Apply the formatter to obtain the value
         Lookup the corresponding output field
         Assign the formatted value to the output field

You may go the other way round too, by deciding that the code is the single source of truth and you generate a file directly out of the code. This won't work if your code is mostly made of a lot of IF statements. Being able to generate a readable file from the code imposes a generic structure to the design of the code. Basically the code would embed the equivalent of the former spreadsheet file, but hardcoded as a dictionary, e.g. in a `Map` in Java.

This data structure can then be exported as a file in various formats (`xls`, `csv`, `xml`, `json`...) for non-developers audiences.

## Integrated Documentation for developers, Living Glossary for other stakeholders

Do you really need to produce Javadoc reports? It's so easy to browse the code in your IDE that you probably won't use the Javadoc reports much. The Javadoc reports are now available directly at your fingertips in your IDE. The same is true for UML class diagrams of classes and their type hierarchies. All this is already Integrated Documentation built-in your editors.

If you really need a reference to give access to the concepts to non-developers, you may introduce a Living Glossary. It scans the code in the `/domain` package to generate a markdown and HTML glossary of all the business domain concepts in the code, extracted from classes, interfaces, enum constants and perhaps some methods names and Javadoc comments. Of course, for the glossary to be good you'll probably have to review and fix many of these Javadoc comments.

## Living Diagram to show the design intent

If the internal design follows a known structure like the *Hexagonal Architecture*, we can make it visible thanks to naming conventions of the corresponding modules. This naming convention and the name of the structure must be documented in the `README` file:

     The design of this module follow the Hexagonal Architecture pattern (link to a reference on the web).

     By convention, the domain model code is in the src/*/domain*/ package, and the rest is all infrastructure code

That's Ready-Made documentation, again.

You may include a link to the domain model package, but it has to survive refactoring changes like moving the domain folder into another folder; to make the link more stable we can make it a Bookmarked Search directly based on the naming convention as a regular expression: `src/*/domain*/`

## Contact information, and Guided Tour

Who should I contact for questions? Just check the service registry, *Consul* in our case, which should have this information as required by the company architects.

A Guided Tour just for the batch is not that difficult to create with a custom annotation, but it may not be very useful for developers. The batch is built with the very standard and well-documented `Spring Batch` framework. This framework controls completely the way the processing takes place. We can safely assume that every developer knows about this framework and the way it works, or that they can learn about it from the standard documentation and tutorials. No need to create any additional custom Guided Tour for that.

## Micro-services Big Picture

How does our Data Pump module fit within the bigger system made of many micro-services? Answering this question takes more effort. One approach would be to regularly run a Journey Test (End-to-End Scenario going through a large number of components of the system) on some environment with distributed tracing enabled. Tools like *Selenium* for running the test and *Zipkin* for distributed tracing may come to mind. You could then visualize the distributed traces to produce another kind of Guided Tour revealing the big picture along each Journey test.
